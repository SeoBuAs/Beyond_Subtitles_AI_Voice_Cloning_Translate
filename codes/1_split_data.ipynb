{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 118473,
     "status": "ok",
     "timestamp": 1747029584620,
     "user": {
      "displayName": "서준혁",
      "userId": "09664879719147401659"
     },
     "user_tz": -540
    },
    "id": "w7k64djSC7VF",
    "outputId": "8998466b-427f-4346-e1f7-85f2cc5777f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.check_call(['pip', 'install', 'moviepy', 'demucs', 'pydub', 'dotenv', 'openai', 'noisereduce', 'pyannote.audio', 'openai-whisper'],\n",
    "                      stdout=subprocess.DEVNULL,\n",
    "                      stderr=subprocess.DEVNULL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21477,
     "status": "ok",
     "timestamp": 1747029606099,
     "user": {
      "displayName": "서준혁",
      "userId": "09664879719147401659"
     },
     "user_tz": -540
    },
    "id": "Bt6FLtyUDBzZ",
    "outputId": "b6b0fa66-39f8-4093-dc40-9ee738a052dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['apt-get', 'install', '-y', 'espeak'], returncode=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "subprocess.run(['apt-get', 'update'], env={'LC_ALL': 'C.UTF-8'})\n",
    "subprocess.run(['apt-get', 'install', '-y', 'espeak'], env={'LC_ALL': 'C.UTF-8'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28086,
     "status": "ok",
     "timestamp": 1747029634184,
     "user": {
      "displayName": "서준혁",
      "userId": "09664879719147401659"
     },
     "user_tz": -540
    },
    "id": "X806ETudDE6_",
    "outputId": "aef64d12-63c8-4e58-f4fd-6e9b4ab66a1d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if event.key is 'enter':\n",
      "\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _speechbrain_save\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _speechbrain_load\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for save\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for load\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _save\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _recover\n"
     ]
    }
   ],
   "source": [
    "# Video processing and editing libraries\n",
    "import cv2\n",
    "from moviepy.editor import VideoFileClip, ImageSequenceClip, AudioClip, concatenate_videoclips, ColorClip\n",
    "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
    "\n",
    "# Audio processing libraries\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "import soundfile as sf\n",
    "\n",
    "# Models and audio source separation / ASR\n",
    "import whisper\n",
    "from demucs.pretrained import get_model\n",
    "from demucs.apply import apply_model\n",
    "\n",
    "# Speech analysis and forced alignment\n",
    "from pyannote.audio import Pipeline\n",
    "from pyannote.audio.pipelines.utils.hook import ProgressHook\n",
    "\n",
    "# OpenAI API and environment variable handling\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Memory management\n",
    "import gc\n",
    "\n",
    "# Ignore Warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzBy_YB_DZeh"
   },
   "source": [
    "# Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dG_HYhKUDOzT"
   },
   "outputs": [],
   "source": [
    "def get_normalized_audio(video_path):\n",
    "    audio_segment = AudioSegment.from_file(video_path, format=\"mp4\")\n",
    "    audio_samples = np.array(audio_segment.get_array_of_samples())\n",
    "\n",
    "    if audio_segment.channels == 2:\n",
    "        audio_samples = audio_samples.reshape((-1, 2))\n",
    "\n",
    "    sample_width = audio_segment.sample_width\n",
    "    max_val = float(2 ** (8 * sample_width - 1))  # 2^(16-1)=32768\n",
    "    audio_float = audio_samples.astype(np.float32) / max_val\n",
    "\n",
    "    del audio_samples, audio_segment\n",
    "    gc.collect()\n",
    "\n",
    "    return audio_float\n",
    "\n",
    "video_path = '''/content/Why you little ( Bart ).mp4'''\n",
    "audio = get_normalized_audio(video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eWTjaVNBDbX0"
   },
   "source": [
    "# Demucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EecdsfWpDVet"
   },
   "outputs": [],
   "source": [
    "def separate_vocals_and_background(audio, device):\n",
    "    audio_tensor = torch.tensor(audio.T).float().unsqueeze(0).to(device)\n",
    "    model = get_model('htdemucs').to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        estimates = apply_model(model, audio_tensor, shifts=1, overlap=0.5)\n",
    "\n",
    "    estimates_np = estimates.squeeze(0).cpu().numpy()\n",
    "\n",
    "    vocals = estimates_np[3].T\n",
    "    background = (estimates_np[0] + estimates_np[1] + estimates_np[2]).T\n",
    "\n",
    "    del audio_tensor, estimates, estimates_np, model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return vocals, background\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vocals, background = separate_vocals_and_background(audio, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mscm1BWNETs8"
   },
   "source": [
    "# Save Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 371,
     "status": "ok",
     "timestamp": 1747030090746,
     "user": {
      "displayName": "서준혁",
      "userId": "09664879719147401659"
     },
     "user_tz": -540
    },
    "id": "0dewp2q4EVEx",
    "outputId": "6b18384a-47e7-4d9d-bb83-74bd623fe912"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocals_file_root = \"/content/vocals.wav\"\n",
    "background_file_root = \"/content/background.wav\"\n",
    "\n",
    "sf.write(vocals_file_root, vocals, 44100)\n",
    "sf.write(background_file_root, background, 44100)\n",
    "\n",
    "del vocals, audio\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0UsyERqDgrJ"
   },
   "source": [
    "# Speech-To-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16932,
     "status": "ok",
     "timestamp": 1747030107687,
     "user": {
      "displayName": "서준혁",
      "userId": "09664879719147401659"
     },
     "user_tz": -540
    },
    "id": "ReKGJz_TDjwo",
    "outputId": "b51f36ba-e47e-4044-e9a6-7f935ca4b9c5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.1.3 to v2.5.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/pyannote/models--pyannote--segmentation/snapshots/059e96f964841d40f1a5e755bb7223f76666bba4/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.7.1, yours is 2.6.0+cu124. Bad things might happen unless you revert torch to 1.x.\n",
      "[00.35 ~ 02.48] You're welcome to watch anything you want on TV.\n",
      "[03.08 ~ 03.78] TV sucks.\n",
      "[04.54 ~ 07.79] I know you're upset right now, so I'll pretend you didn't say that.\n",
      "[09.44 ~ 09.98] You little...\n",
      "[09.98 ~ 10.04] Yow!\n",
      "[12.45 ~ 13.38] You little...\n"
     ]
    }
   ],
   "source": [
    "def speech_to_text_with_vad(audio_file, device='cuda'):\n",
    "    with torch.no_grad():\n",
    "      model = whisper.load_model(\"medium\", device=device)\n",
    "      vad_pipeline = Pipeline.from_pretrained(\"pyannote/voice-activity-detection\", use_auth_token='')\n",
    "      vad_pipeline.to(torch.device(\"cuda\"))\n",
    "      result = model.transcribe(audio_file, word_timestamps=True)\n",
    "\n",
    "    vad_output = vad_pipeline(audio_file)\n",
    "\n",
    "    refined_segments = []\n",
    "    for segment in result['segments']:\n",
    "        vad_matches = [\n",
    "            (speech.start, speech.end)\n",
    "            for speech in vad_output.get_timeline()\n",
    "            if (speech.start <= segment['end'] and speech.end >= segment['start'])\n",
    "        ]\n",
    "\n",
    "        if vad_matches:\n",
    "            best_match = min(vad_matches, key=lambda x: x[1] - x[0])\n",
    "            refined_segments.append({\n",
    "                'start': max(segment['start'], best_match[0]),\n",
    "                'end': min(segment['end'], best_match[1]),\n",
    "                'text': segment['text'].strip()\n",
    "            })\n",
    "        else:\n",
    "            refined_segments.append(segment)\n",
    "\n",
    "    output_string = \"\"\n",
    "    for segment in refined_segments:\n",
    "        output_string += f\"[{segment['start']:05.2f} ~ {segment['end']:05.2f}] {segment['text']}\\n\"\n",
    "\n",
    "    del model, vad_pipeline, result, vad_output\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return {\n",
    "        'segments': refined_segments,\n",
    "        'output_string': output_string\n",
    "    }\n",
    "\n",
    "result = speech_to_text_with_vad(vocals_file_root, 'cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hEqoyLBD9-Y"
   },
   "source": [
    "# Split Audio and Excel Merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1747030107688,
     "user": {
      "displayName": "서준혁",
      "userId": "09664879719147401659"
     },
     "user_tz": -540
    },
    "id": "L5AKsiHUD_8-",
    "outputId": "e706ec98-683f-40b8-d28f-84a37ccd3282"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio segments saved to: vocals_segments\n",
      "CSV file saved to: vocals_segments/vocals_segments_M.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from pydub import AudioSegment\n",
    "\n",
    "suffix = 'M'\n",
    "\n",
    "def split_audio_by_segments(audio_file, segments, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    base_filename = os.path.basename(audio_file)\n",
    "    file_name_without_ext = os.path.splitext(base_filename)[0]\n",
    "\n",
    "    try:\n",
    "        audio = AudioSegment.from_file(audio_file)\n",
    "        use_pydub = True\n",
    "    except:\n",
    "        audio_data, sample_rate = librosa.load(audio_file, sr=None)\n",
    "        use_pydub = False\n",
    "\n",
    "    csv_data = []\n",
    "\n",
    "    for i, segment in enumerate(segments):\n",
    "        start_time = segment['start']\n",
    "        end_time = segment['end']\n",
    "        text = segment['text']\n",
    "\n",
    "        segment_filename = f\"{file_name_without_ext}_{i+1:03d}_{suffix}.wav\"\n",
    "        segment_path = os.path.join(output_dir, segment_filename)\n",
    "\n",
    "        if use_pydub:\n",
    "            start_ms = int(start_time * 1000)\n",
    "            end_ms = int(end_time * 1000)\n",
    "            segment_audio = audio[start_ms:end_ms]\n",
    "            segment_audio.export(segment_path, format=\"wav\")\n",
    "        else:\n",
    "            start_sample = int(start_time * sample_rate)\n",
    "            end_sample = int(end_time * sample_rate)\n",
    "            segment_samples = audio_data[start_sample:end_sample]\n",
    "            sf.write(segment_path, segment_samples, sample_rate)\n",
    "\n",
    "        csv_data.append({\n",
    "            'filename': segment_filename,\n",
    "            'start_time': start_time,\n",
    "            'end_time': end_time,\n",
    "            'duration': end_time - start_time,\n",
    "            'text': text\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(csv_data)\n",
    "    csv_path = os.path.join(output_dir, f\"{file_name_without_ext}_segments_{suffix}.csv\")\n",
    "    df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    return csv_path\n",
    "\n",
    "output_directory = \"vocals_segments\"\n",
    "csv_file_path = split_audio_by_segments(vocals_file_root, result[\"segments\"], output_directory)\n",
    "\n",
    "print(f\"Audio segments saved to: {output_directory}\")\n",
    "print(f\"CSV file saved to: {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1741,
     "status": "ok",
     "timestamp": 1747030116959,
     "user": {
      "displayName": "서준혁",
      "userId": "09664879719147401659"
     },
     "user_tz": -540
    },
    "id": "Q950VQB0aTl5",
    "outputId": "fd83f3bc-7280-4b3c-88dc-dbe1e39f94aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "def zip_folder(folder_path, output_path):\n",
    "    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, dirs, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                arcname = os.path.relpath(file_path, os.path.dirname(folder_path))\n",
    "                zipf.write(file_path, arcname)\n",
    "    return True\n",
    "\n",
    "folder_to_zip = \"/content/vocals_segments\"\n",
    "zip_file_path = \"/content/vocals_segments.zip\"\n",
    "\n",
    "zip_folder(folder_to_zip, zip_file_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOvOJWxSUFMtpw2UHGdzozm",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
